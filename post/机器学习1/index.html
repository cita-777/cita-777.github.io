<!doctype html><html lang=zh-CN data-theme=light><head><meta charset=UTF-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: light)"><meta name=generator content="Hugo 0.145.0"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="机器学习(1)"><meta itemprop=description content="机器学习(1)"><meta name=description content="机器学习(1)"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="/imgs/cita_avatar.jpg"><meta itemprop=keywords content="机器学习"><meta property="og:type" content="article"><meta property="og:title" content="机器学习(1)"><meta property="og:description" content="机器学习(1)"><meta property="og:image" content="/imgs/cita_avatar.jpg"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/"><meta property="og:site_name" content="Cita's blog"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Cita"><meta property="article:published_time" content="2025-03-21 00:00:00 +0000 UTC"><meta property="article:modified_time" content="2025-03-21 00:00:00 +0000 UTC"><link type=text/css rel=stylesheet href=/js/3rd/font-awesome/6.7.2/css/all.min.css><link type=text/css rel=stylesheet href=/js/3rd/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=/js/3rd/viewerjs/1.11.6/viewer.min.css><link rel=stylesheet href="/css/main.min.css?=1751594388"><style type=text/css>.post-footer hr:after{content:"~ 我可是有底线的哟 ~"}.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>机器学习(1) - Cita's blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Cita's blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Cita 的博客</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about.html class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-flinks"><a href=/flinks.html class=hvr-icon-pulse rel=section><i class="fa fa-thumbs-up hvr-icon"></i>常用学习/工具网站</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档
<span class=badge>16</span></a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#前言><em><strong>前言</strong></em></a></li><li><a href=#介绍>介绍</a><ul><li><a href=#什么是机器学习>什么是机器学习</a></li><li><a href=#监督学习>监督学习</a></li><li><a href=#无监督学习>无监督学习</a></li></ul></li><li><a href=#模型和成本函数>模型和成本函数</a><ul><li><a href=#模型表示>模型表示</a></li><li><a href=#成本函数>成本函数</a></li></ul></li><li><a href=#参数学习>参数学习</a><ul><li><a href=#梯度下降>梯度下降</a></li></ul></li><li><a href=#多元线性回归>多元线性回归</a><ul><li><a href=#引入多元>引入多元</a></li><li><a href=#对于多元的梯度下降>对于多元的梯度下降</a></li><li><a href=#实践中的梯度下降---特征缩放>实践中的梯度下降 - 特征缩放</a></li><li><a href=#实践中的梯度下降---学习率>实践中的梯度下降 - 学习率</a></li></ul></li><li><a href=#参数计算>参数计算</a><ul><li><a href=#正规方程>正规方程</a></li><li><a href=#正规方程的推导过程>正规方程的推导过程</a></li></ul></li><li><a href=#logistic-regression>Logistic Regression</a><ul><li><a href=#二分类>二分类</a></li><li><a href=#成本函数-1>成本函数</a></li><li><a href=#梯度下降-1>梯度下降</a></li></ul></li><li><a href=#正则化>正则化</a><ul><li><a href=#加入正则化的成本函数>加入正则化的成本函数</a></li><li><a href=#加入正则化的线性回归>加入正则化的线性回归</a><ul><li><a href=#梯度下降-2>梯度下降</a></li><li><a href=#正规方程-1>正规方程</a></li></ul></li><li><a href=#加入正则化的-logistic-regression>加入正则化的 Logistic Regression</a><ul><li><a href=#成本函数-2>成本函数</a></li><li><a href=#梯度下降-3>梯度下降</a></li></ul></li></ul></li><li><a href=#应用机器学习的建议>应用机器学习的建议</a><ul><li><a href=#决定下一步做什么>决定下一步做什么</a></li><li><a href=#模型选择和训练验证测试集>模型选择和训练、验证、测试集</a></li><li><a href=#诊断偏差和方差>诊断偏差和方差</a></li><li><a href=#正则化和偏差方差>正则化和偏差、方差</a></li></ul></li><li><a href=#机器学习系统设计>机器学习系统设计</a><ul><li><a href=#误差分析>误差分析</a></li><li><a href=#不对称性分类的误差评估>不对称性分类的误差评估</a></li></ul></li><li><a href=#支持向量机>支持向量机</a><ul><li><a href=#直观理解>直观理解</a></li><li><a href=#数学原理>数学原理</a></li><li><a href=#核函数>核函数</a><ul><li><a href=#思想>思想</a></li><li><a href=#选取标记点>选取标记点</a></li><li><a href=#参数选择>参数选择</a></li></ul></li><li><a href=#使用-svm>使用 SVM</a></li></ul></li><li><a href=#无监督学习-1>无监督学习</a><ul><li><a href=#k-means-算法>K-Means 算法</a><ul><li><a href=#优化目标>优化目标</a></li><li><a href=#随机初始化>随机初始化</a></li><li><a href=#选择聚类数量>选择聚类数量</a></li></ul></li></ul></li><li><a href=#降维>降维</a><ul><li><a href=#主成分分析pca>主成分分析（PCA）</a><ul><li><a href=#选取-k-值>选取 k 值</a></li></ul></li><li><a href=#压缩重现>压缩重现</a></li><li><a href=#应用-pca-的建议>应用 PCA 的建议</a></li></ul></li><li><a href=#异常检测>异常检测</a><ul><li><a href=#高斯分布>高斯分布</a></li><li><a href=#算法>算法</a></li><li><a href=#开发和评估异常检测系统>开发和评估异常检测系统</a></li><li><a href=#异常检测-vs-监督学习>异常检测 VS 监督学习</a></li><li><a href=#选择使用的特征>选择使用的特征</a></li><li><a href=#多元高斯分布>多元高斯分布</a></li></ul></li><li><a href=#推荐系统>推荐系统</a><ul><li><a href=#基于内容的推荐算法>基于内容的推荐算法</a></li><li><a href=#协同过滤基本思想>协同过滤基本思想</a></li><li><a href=#协同过滤算法>协同过滤算法</a></li><li><a href=#矢量化低秩矩阵分解>矢量化：低秩矩阵分解</a></li><li><a href=#实施细节均值规范化>实施细节：均值规范化</a></li></ul></li><li><a href=#大规模机器学习>大规模机器学习</a><ul><li><a href=#随机梯度下降>随机梯度下降</a></li><li><a href=#mini-batch-梯度下降>Mini-batch 梯度下降</a></li><li><a href=#在线学习>在线学习</a></li><li><a href=#mapreduce>MapReduce</a></li></ul></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=Cita src=/imgs/img-lazy-loading.gif data-src=/imgs/cita_avatar.jpg><p class=site-author-name itemprop=name>Cita</p><div class=site-description itemprop=description>分享技术与生活的点滴。</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>16</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>6</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>29</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/cita-777 title="Github → https://github.com/cita-777" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>
Github
</a></span><span class=links-of-social-item><a href=mailto:juricek.chen@gmail.com title="G-Mail → mailto:juricek.chen@gmail.com" rel=noopener class=hvr-icon-pulse target=_blank><i class="fa fa-envelope fa-fw hvr-icon"></i>
G-Mail</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate="2024-10-06 00:00:00 +0000 UTC"></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>总访客数：</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>页面浏览：</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=86693></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=182></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate="2025-03-21 00:00:00 +0000 UTC"></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=goto-comments class="button goto-comments" title=直达评论><i class="fas fa-comments"></i></div><div id=goto-i18n-translate class=button title=多语言翻译><i class="fas fa-globe"></i></div><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><a role=button class="book-mark-link book-mark-link-fixed"></a><a href=https://github.com/cita-777 rel="noopener external nofollow noreferrer" target=_blank title="Follow me on GitHub" class="exturl github-corner"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/cita_avatar.jpg"><meta itemprop=name content="Cita"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="Cita"><meta itemprop=description content="分享技术与生活的点滴。"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="机器学习(1)"><meta itemprop=description content="机器学习(1)"></span><header class=post-header><h1 class=post-title itemprop="name headline">机器学习(1)
<a href=https://github.com/cita-777/blog.source/tree/main/content/post/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%281%29/index.md rel="noopener external nofollow noreferrer" target=_blank class="exturl post-edit-link" title=编辑><i class="fa fa-pen-nib"></i></a></h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="fas fa-solid fa-calendar"></i>
</span><span class=post-meta-item-text title=发表于>发表于：
</span><time title="创建时间：2025年03月21日 00:00:00 CST" itemprop="dateCreated datePublished" datetime="2025-03-21 00:00:00 +0000 UTC">2025年03月21日
</time></span><span class=post-meta-item><span class=post-meta-item-icon><i class="fas fa-solid fa-folder-open"></i>
</span><span class=post-meta-item-text title=分类于>分类于：
</span><span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/ itemprop=url rel=index><span itemprop=name>人工智能</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="fas fa-solid fa-file-word"></i>
</span><span class=post-meta-item-text>字数：</span>
<span>9314</span>
</span><span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="fas fa-solid fa-clock"></i>
</span><span class=post-meta-item-text>阅读：&ap;</span>
<span>19分钟</span>
</span><span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="fas fa-solid fa-eye"></i>
</span><span class=post-meta-item-text>浏览：
</span><span id=pageview-count data-path=/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/><i class="fa fa-sync fa-spin"></i>
</span></span><span class=post-meta-item title><span class=post-meta-item-icon><i class="fas fa-solid fa-comments"></i>
</span><span class=post-meta-item-text title=评论>评论：
</span><span id=comments-count data-path=/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class=post-body itemprop=articleBody><div class=post-expired-tip id=post-expired-tip><div class=post-expired-title><i class="fa-solid fa-hourglass-half"></i>
<span>温馨提醒</span></div><div id=post-expired-content class=post-expired-content></div></div><div class=post-summary-wrapper><div class=summary-title><span><i class="fa-solid fa-list"></i></span>
<span>总结摘要</span></div><div class=summary-content>机器学习(1)</div></div><a id=more></a><h2 id=前言><em><strong>前言</strong></em>
<a class=header-anchor href=#%e5%89%8d%e8%a8%80></a></h2><p>吴恩达机器学习回顾使用</p><div class=bilibili-video><iframe src="//player.bilibili.com/player.html?bvid=BV164411b7dx&page=1" scrolling=no border=0 frameborder=no framespacing=0 allowfullscreen></iframe></div><h2 id=介绍>介绍
<a class=header-anchor href=#%e4%bb%8b%e7%bb%8d></a></h2><h3 id=什么是机器学习>什么是机器学习
<a class=header-anchor href=#%e4%bb%80%e4%b9%88%e6%98%af%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0></a></h3><p>对于机器学习的两种定义：</p><ul><li>&ldquo;the field of study that gives computers the ability to learn without being explicitly programmed.&rdquo; - Arthur Samuel</li><li>&ldquo;A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.&rdquo; - Tom Mitchell</li></ul><p>任何机器学习问题可以归于两大类之一：<strong>监督学习（Supervised Learning）</strong>，或<strong>无监督学习（Unsupervised Learning，或称非监督学习）</strong>。当然，还有半监督学习、强化学习等，尚不在讨论范围内。</p><h3 id=监督学习>监督学习
<a class=header-anchor href=#%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0></a></h3><p>监督学习使用有标签的数据集，其任务是学习一个模型，使模型能够对任意给定的输入，对其相应的输出做出一个好的预测。</p><p>监督学习问题被分为**回归（Regression）<strong>和</strong>分类（Classification）**问题：</p><ul><li>回归问题：将输入变量映射到某个连续函数，即预测一个连续值；</li><li>分类问题：将输入变量映射到离散的类别中，即预测一个离散值。</li></ul><h3 id=无监督学习>无监督学习
<a class=header-anchor href=#%e6%97%a0%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0></a></h3><p>在无监督学习中，使用的数据集没有标签，不知道结果会是什么样子，但可以通过聚类的方式从数据中提取一个特殊的结构。</p><h2 id=模型和成本函数>模型和成本函数
<a class=header-anchor href=#%e6%a8%a1%e5%9e%8b%e5%92%8c%e6%88%90%e6%9c%ac%e5%87%bd%e6%95%b0></a></h2><h3 id=模型表示>模型表示
<a class=header-anchor href=#%e6%a8%a1%e5%9e%8b%e8%a1%a8%e7%a4%ba></a></h3><p>为了描述监督学习问题，我们的目标是，通过一个训练集，学习一个函数 $h : X \rightarrow Y$，使得 $h(x)$ 对于对应值 $y$ 是一个很好的预测器。由于历史原因，$h(x)$ 被称为假设函数（hypothesis function）。</p><h3 id=成本函数>成本函数
<a class=header-anchor href=#%e6%88%90%e6%9c%ac%e5%87%bd%e6%95%b0></a></h3><p>**成本函数（cost function）**用于测量假设函数的准确度，即模型预测的好坏：</p><p>$$J(\theta_0, \theta_1) = \frac{1}{2m}\sum^m_{i=1}(\hat y_i - y_i)^2 = \frac{1}{2m}\sum^m_{i=1}(h_{\theta}(x_i) - y_i)^2$$</p><p>这个函数也被称为“平方误差函数（Squared error function）”或者“均方误差（Mean squared error）”。在取平均时多了一个 $\frac{1}{2}$，这是为了方便计算梯度下降，求导时 $\frac{1}{2}$ 将被消掉。</p><h2 id=参数学习>参数学习
<a class=header-anchor href=#%e5%8f%82%e6%95%b0%e5%ad%a6%e4%b9%a0></a></h2><h3 id=梯度下降>梯度下降
<a class=header-anchor href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d></a></h3><p>在已有假设函数和成本函数的情况下，用**梯度下降（Gradient Descent）**可以估计得到假设函数中的参数。迭代使用：</p><p>$$\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j}J(\theta_0, \theta_1)$$</p><p>直到收敛或到达预定的迭代次数。</p><p>其中，$j = 0,1$ 表示特征的 index，$\alpha$ 为学习率。</p><h2 id=多元线性回归>多元线性回归
<a class=header-anchor href=#%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92></a></h2><h3 id=引入多元>引入多元
<a class=header-anchor href=#%e5%bc%95%e5%85%a5%e5%a4%9a%e5%85%83></a></h3><p>引入以下符号：</p><ul><li>$x_j^{(i)}$：第 $i$ 个训练样本的特征 $j$ 的值</li><li>$x^{(i)}$：第 $i$ 个训练样本的所有特征</li><li>$m$：训练样本的数量</li><li>$n$：特征的数量</li></ul><p>则有多项式的假设函数：</p><p>$$h_{\theta}(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + &mldr; + \theta_n x_n$$</p><p>令 $x_0 = 1$，则上式可以简写为：</p><p>$$h_{\theta}(x) = \big[\theta_0\ \ \theta_1\ \ &mldr; \ \ \theta_n\big] \left[\begin{matrix}x_0\\ x_1\\ &mldr;\\ x_n\end{matrix}\right]= \theta^Tx$$</p><h3 id=对于多元的梯度下降>对于多元的梯度下降
<a class=header-anchor href=#%e5%af%b9%e4%ba%8e%e5%a4%9a%e5%85%83%e7%9a%84%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d></a></h3><p>同理，有</p><p>$$\theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j}J(\theta)$$</p><p>当 n >= 1 时，有</p><p>$$\frac{\partial}{\partial \theta_j}J(\theta) = \frac{1}{m}\sum^m_{i=1}(h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}_j$$</p><p>注意以上等式由</p><p>$$J(\theta) = \frac{1}{2m}\sum^m_{i=1}(h_{\theta}(x^{(i)}) - y^{(i)})^2$$</p><p>推得。</p><h3 id=实践中的梯度下降---特征缩放>实践中的梯度下降 - 特征缩放
<a class=header-anchor href=#%e5%ae%9e%e8%b7%b5%e4%b8%ad%e7%9a%84%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d---%e7%89%b9%e5%be%81%e7%bc%a9%e6%94%be></a></h3><p>可以通过使每个输入值在大致相同的范围内来加速梯度下降。这是因为梯度在小范围内下降快，而在大范围内下降较慢；另外，对于不平整的变量，梯度在下降至最优值的过程中会出现降低效率的震荡。</p><p>因此将**特征缩放（feature scaling）<strong>和</strong>均值归一化（mean normalization）**两种技术结合使用。<strong>特征缩放</strong>将输入值除以输入变量的范围（即最大值减去最小值），从而得到一个大小为 1 的新范围；<strong>均值归一化</strong>涉及从输入变量的值减去输入变量的平均值，从而导致输入变量的新平均值为 0。公式为：</p><p>$$x_i := \frac{x_i - \mu_i}{s_i}$$</p><p>其中，$\mu_i$ 是特征 $i$ 所有值的平均值，$s_i$ 是特征 $i$ 的范围（最大值减最小值）。</p><h3 id=实践中的梯度下降---学习率>实践中的梯度下降 - 学习率
<a class=header-anchor href=#%e5%ae%9e%e8%b7%b5%e4%b8%ad%e7%9a%84%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d---%e5%ad%a6%e4%b9%a0%e7%8e%87></a></h3><p>当学习率 $\alpha$ 太小，则梯度下降太慢；而当学习率 $\alpha$ 太大，则梯度可能不会下降，也因此不会收敛。因此有时需要观察并进行调整。</p><h2 id=参数计算>参数计算
<a class=header-anchor href=#%e5%8f%82%e6%95%b0%e8%ae%a1%e7%ae%97></a></h2><h3 id=正规方程>正规方程
<a class=header-anchor href=#%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b></a></h3><p>梯度下降是最小化 $J$ 的一种方法。第二种方法是<strong>正规方程（Normal Equation）</strong>，它显式地执行最小化，而不使用迭代式的算法。</p><p>$$\theta = (X^TX)^{-1}X^Ty$$</p><p>正规方程方法中，无需做特征缩放。两种方法的对比如下：</p><table><thead><tr><th style=text-align:center>梯度下降</th><th style=text-align:center>正规方程</th></tr></thead><tbody><tr><td style=text-align:center>需要选择学习率</td><td style=text-align:center>不需要选择学习率</td></tr><tr><td style=text-align:center>需要多次迭代</td><td style=text-align:center>不需要迭代</td></tr><tr><td style=text-align:center>$O(kn^2)$</td><td style=text-align:center>$O(n^3)$，需要计算 $(X^TX)^{-1}$</td></tr><tr><td style=text-align:center>当 n 较大时效果很好</td><td style=text-align:center>当 n 较大时速度较慢</td></tr></tbody></table><p>不过正规方程方法要求 $X^TX$ 可逆。$X^TX$ 不可逆的原因有两种可能：</p><ol><li>列向量线性相关：即训练集中存在冗余特征（特征线性依赖），此时应该剔除掉多余特征；</li><li>特征过多（多于样本数量）：此时应该去掉影响较小的特征，或引入正则化（regularization）项。</li></ol><h3 id=正规方程的推导过程>正规方程的推导过程
<a class=header-anchor href=#%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e7%9a%84%e6%8e%a8%e5%af%bc%e8%bf%87%e7%a8%8b></a></h3><p>把数据集表示为矩阵</p><p>$$X = \left( \begin{matrix} x_{11} & x_{12} & \cdots & x_{1d} & 1 \\ x_{21} & x_{22} & \cdots & x_{2d} & 1 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ x_{m1} & x_{m2} & \cdots & x_{md} & 1 \\ \end{matrix} \right) = \left( \begin{matrix} x_{1}^T & 1 \\ x_{2}^T & 1 \\ \vdots & \vdots \\ x_{m}^T & 1 \\ \end{matrix} \right)$$</p><p>同时将标签也写成向量形式</p><p>$$y = (y_1;y_2;&mldr;;y_m)$$</p><p>由均方误差最小化，可得</p><p>$$\theta^* = arg_{\theta}min(y-X\theta)^T(y-X\theta)$$</p><p>其中，$\theta^*$表示 $\theta$ 的解。</p><p>令</p><p>$$E_{\theta} = (y-X\theta)^T(y-X\theta)$$</p><p>对 $\theta$ 求导得到</p><p>$$
\begin{equation}
\begin{split}
\frac{\partial E_{\theta}}{\partial \theta}&=-X^T(y-X\theta) + (y^T - \theta^TX^T) \cdot (-X)\\
&=2X^T(X\theta - y)
\end{split}
\end{equation}
$$</p><p>令上式为 0，有</p><p>$$2X^T(X\theta - y) = 0$$</p><p>$$X^TX\theta = X^Ty$$</p><p>最终得到</p><p>$$\theta = (X^TX)^{-1}X^Ty$$</p><p>当 $X^TX$ 不为满秩矩阵（不可逆）时，可解出多个 $\theta$ 使均方误差最小化。因此将由学习算法的归纳偏好来决定选择哪一个解作为输出，常见的做法就是引入正则化项。</p><h2 id=logistic-regression>Logistic Regression
<a class=header-anchor href=#logistic-regression></a></h2><h3 id=二分类>二分类
<a class=header-anchor href=#%e4%ba%8c%e5%88%86%e7%b1%bb></a></h3><p>$$h_{\theta} = g(\theta^Tx)$$</p><p>$$z = \theta^Tx$$</p><p>$$g(z) = \frac{1}{1 + e^{-z}}$$</p><p>其中，$g(z)$ 被称为 Sigmoid 函数（或者 Logistic 函数），将任意实数映射到 (0, 1) 区间。这样，可以通过划分判定边界（decision boundary）进行分类。</p><h3 id=成本函数-1>成本函数
<a class=header-anchor href=#%e6%88%90%e6%9c%ac%e5%87%bd%e6%95%b0-1></a></h3><p>Logistic Regression 不能使用和线性回归相同的成本函数，因为 Sigmoid 函数是非凸的，容易陷入局部最优。</p><p>作为替代，使用以下成本函数：</p><p>$$J(\theta) = \frac{1}{m}\sum^m_{i=1}Cost(h_{\theta}(x^{(i)}), y^{(i)})$$</p><p>$$Cost(h_{\theta}(x), y) = -y log(h_{\theta}(x)) - (1-y)log(1-h_{\theta}(x))$$</p><p>注意，$y$ 只有 0 或 1 两种取值。</p><p>向量化的表示为：</p><p>$$h = g(X\theta)$$</p><p>$$J(\theta) = \frac{1}{m} \cdot (-y^Tlog(h) - (1-y)^Tlog(1-h))$$</p><h3 id=梯度下降-1>梯度下降
<a class=header-anchor href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d-1></a></h3><p>由梯度下降的一般形式：</p><p>$$\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j}J(\theta)$$</p><p>可以推导出 LR 的梯度下降公式：</p><p>$$\theta_j := \theta_j - \frac{\alpha}{m} \sum^m_{i=1}(h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$$</p><p>向量化表示为：</p><p>$$\theta := \theta - \frac{\alpha}{m}X^T(g(X\theta) - \vec y)$$</p><p>推导过程如下：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://raw.githubusercontent.com/bighuang624/pic-repo/master/derivative-of-sigmoid-function.png alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://raw.githubusercontent.com/bighuang624/pic-repo/master/Partial-derivative-of-J.png alt></p><h2 id=正则化>正则化
<a class=header-anchor href=#%e6%ad%a3%e5%88%99%e5%8c%96></a></h2><p>**高偏差（high bias）<strong>或者欠拟合是指模型对已有数据的拟合较差，通常因为模型太简单或者使用过少的特征；而</strong>高方差（high variance）**或者过拟合是指模型对已有数据的拟合过强，以至于对于新数据无法很好的预测，通常因为模型过于复杂。</p><p>两种技术可用于处理过拟合问题：</p><ol><li>减少特征数量：人工选择留下的特征，或者使用特征选择的算法；</li><li><strong>正则化（Regularization）</strong>：留下所有的特征，但是减小参数 $\theta_j$。</li></ol><h3 id=加入正则化的成本函数>加入正则化的成本函数
<a class=header-anchor href=#%e5%8a%a0%e5%85%a5%e6%ad%a3%e5%88%99%e5%8c%96%e7%9a%84%e6%88%90%e6%9c%ac%e5%87%bd%e6%95%b0></a></h3><p>$$min_{\theta}\frac{1}{2m}\Big[\sum^m_{i=1}(h_{\theta}(x^{(i)}) - y^{(i)})^2 + \lambda \sum^n_{j=1}\theta_j^2\Big]$$</p><p>其中，$\lambda$ 用于控制参数过大所付出的代价。$\lambda$ 选取过大时，可能导致欠拟合。</p><h3 id=加入正则化的线性回归>加入正则化的线性回归
<a class=header-anchor href=#%e5%8a%a0%e5%85%a5%e6%ad%a3%e5%88%99%e5%8c%96%e7%9a%84%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92></a></h3><h4 id=梯度下降-2>梯度下降
<a class=header-anchor href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d-2></a></h4><p>$$\theta_0 := \theta_0 - \alpha \frac{1}{m}\sum^m_{i=1}(h_{\theta}(x^{(i)}) - y^{(i)})x_0^{(i)}$$</p><p>$$\theta_j := \theta_j - \alpha \Big[\Big(\frac{1}{m}\sum^m_{i=1}(h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} \Big) + \frac{\lambda}{m}\theta_j \Big] \ \ \ \ j = 1, 2, &mldr;, n$$</p><h4 id=正规方程-1>正规方程
<a class=header-anchor href=#%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b-1></a></h4><p>$$\theta = (X^TX + \lambda \cdot L)^{-1}X^Ty$$</p><p>$$where \ L=
\begin{bmatrix}
0 & & \\
& 1 & \\
& & \ddots \\
& & & 1
\end{bmatrix}$$</p><h3 id=加入正则化的-logistic-regression>加入正则化的 Logistic Regression
<a class=header-anchor href=#%e5%8a%a0%e5%85%a5%e6%ad%a3%e5%88%99%e5%8c%96%e7%9a%84-logistic-regression></a></h3><h4 id=成本函数-2>成本函数
<a class=header-anchor href=#%e6%88%90%e6%9c%ac%e5%87%bd%e6%95%b0-2></a></h4><p>$$J(\theta) = -\frac{1}{m}\sum^m_{i=1}\Big[y ^{(i)}log(h_{\theta}(x^{(i)})) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)}))\Big] + \frac{\lambda}{2m}\sum^n_{j=1}\theta^2_j$$</p><h4 id=梯度下降-3>梯度下降
<a class=header-anchor href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d-3></a></h4><p>$$\theta_0 := \theta_0 - \alpha \frac{1}{m}\sum^m_{i=1}(h_{\theta}(x^{(i)}) - y^{(i)})x_0^{(i)}$$</p><h2 id=应用机器学习的建议>应用机器学习的建议
<a class=header-anchor href=#%e5%ba%94%e7%94%a8%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%bb%ba%e8%ae%ae></a></h2><h3 id=决定下一步做什么>决定下一步做什么
<a class=header-anchor href=#%e5%86%b3%e5%ae%9a%e4%b8%8b%e4%b8%80%e6%ad%a5%e5%81%9a%e4%bb%80%e4%b9%88></a></h3><p>模型的预测错误可以通过以下方式进行排解：</p><ul><li>更多训练数据：解决高方差</li><li>用更少的特征：解决高方差</li><li>用额外的特征：解决高偏差</li><li>用多项式特征：解决高偏差</li><li>增大或减小正则化参数 $\lambda$：解决高方差或高偏差</li></ul><p>可以通过画出**学习曲线（Learning curves）**来帮助分析。</p><h3 id=模型选择和训练验证测试集>模型选择和训练、验证、测试集
<a class=header-anchor href=#%e6%a8%a1%e5%9e%8b%e9%80%89%e6%8b%a9%e5%92%8c%e8%ae%ad%e7%bb%83%e9%aa%8c%e8%af%81%e6%b5%8b%e8%af%95%e9%9b%86></a></h3><p>假设函数可能在训练集上拟合得很好，但是由于过拟合，在实际使用中表现不佳。因此，我们将数据集划分为训练集、验证集和测试集，以评估假设函数。</p><ul><li>验证集：对应在训练中不断更新的参数和模型准确率；</li><li>测试集：对应超参数和最终的模型准确率。</li></ul><p>或者说，<strong>验证集用于模型选择和调参，而测试集用于估计实际使用时的泛化能力</strong>。这样，我们不需要在利用测试集调参后，又使用测试集来估计泛化能力而得到错误的结果。</p><h3 id=诊断偏差和方差>诊断偏差和方差
<a class=header-anchor href=#%e8%af%8a%e6%96%ad%e5%81%8f%e5%b7%ae%e5%92%8c%e6%96%b9%e5%b7%ae></a></h3><p>偏差（欠拟合）：</p><ul><li>$J_{train}(\theta)$ 较高；</li><li>$J_{train}(\theta) \approx J_{train}(\theta)$。</li></ul><p>方差（过拟合）：</p><ul><li>$J_{train}(\theta)$ 较低；</li><li>$J_{train}(\theta) \geq J_{train}(\theta)$。</li></ul><h3 id=正则化和偏差方差>正则化和偏差、方差
<a class=header-anchor href=#%e6%ad%a3%e5%88%99%e5%8c%96%e5%92%8c%e5%81%8f%e5%b7%ae%e6%96%b9%e5%b7%ae></a></h3><p>$$J(\theta) = \frac{1}{2m}\sum^m_{i=1}(h_{\theta}(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m}\sum^m_{j=1}\theta_j^2$$</p><p>首先考虑不使用正则化，然后选取一系列想要尝试的 $\lambda$ 值，最小化代价函数来得到 $\theta$，然后用验证集来评估，评估时用的代价函数去掉正则项：</p><p>$$J_{cv}(\theta) = \frac{1}{2m_{cv}}\sum^{m_{cv}}_{i=1}(h_{\theta}(x^{(i)}_{cv}) - y_{cv}^{(i)})^2$$</p><p>可以画一幅如下图所示的图像来确定合适的 $\lambda$（实际一般有噪声）：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://raw.githubusercontent.com/bighuang624/pic-repo/master/regularization-bias-and-variance.png alt></p><h2 id=机器学习系统设计>机器学习系统设计
<a class=header-anchor href=#%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%b3%bb%e7%bb%9f%e8%ae%be%e8%ae%a1></a></h2><h3 id=误差分析>误差分析
<a class=header-anchor href=#%e8%af%af%e5%b7%ae%e5%88%86%e6%9e%90></a></h3><p>可以将误分类的验证集数据提出来，看看哪一类误分类比例最高，做出相应的改进。</p><h3 id=不对称性分类的误差评估>不对称性分类的误差评估
<a class=header-anchor href=#%e4%b8%8d%e5%af%b9%e7%a7%b0%e6%80%a7%e5%88%86%e7%b1%bb%e7%9a%84%e8%af%af%e5%b7%ae%e8%af%84%e4%bc%b0></a></h3><p>当正负样本的比例比较悬殊时，单纯使用正确率来评估会使得分类器只预测一种结果。因此，我们可以使用**准确率（Precision）<strong>和</strong>召回率（Recall）**作为评估度量。</p><p>$$P = \frac{TP}{TP+FP}$$</p><p>$$R = \frac{TP}{TP+FN}$$</p><p>两者都较高，说明模型在面对偏斜类时也有很好的表现。但大部分情况下，需要在两者之间做权衡。如果想要结果要较高的确信度，那么将准确率作为最重要的标准；如果不想漏掉样例（例如疾病判断），那么将召回率作为最重要的标准。</p><p>如果想要综合评价，我们可以使用 F1-Score：</p><p>$$F1 = \frac{2PR}{P+R}$$</p><h2 id=支持向量机>支持向量机
<a class=header-anchor href=#%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f%e6%9c%ba></a></h2><p><img src=/imgs/img-lazy-loading.gif data-src=https://raw.githubusercontent.com/bighuang624/pic-repo/master/SVM-1.png alt></p><!--### 优化目标

代价函数：

$$min\_{\theta}C\sum^m\_{i=1}\Big[ y^{(i)}cost\_1(\theta^Tx^{(i)}) + (1-y^{(i)})cost\_0(\theta^Tx^{(i)}) \Big] + \frac{1}{2}\sum^n\_{j=1}\theta^2\_j$$

正则化参数 $C$ 越大，决策边界越会精准分类，因此异常样本造成的影响也会更大。--><h3 id=直观理解>直观理解
<a class=header-anchor href=#%e7%9b%b4%e8%a7%82%e7%90%86%e8%a7%a3></a></h3><p>支持向量机（Support Vector Machine），便是根据训练样本的分布，搜索所有可能的线性分类器中最佳的那个。因为决定直线位置的并非所有的训练数据，而是其中的两个空间<strong>间隔（margin）<strong>最小的两个不同类别的数据点，这种可以用来真正帮助决策最优线性分类模型的数据点叫做</strong>支持向量（support vector）</strong>。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://raw.githubusercontent.com/bighuang624/pic-repo/master/margin-and-support-vector.png alt></p><h3 id=数学原理>数学原理
<a class=header-anchor href=#%e6%95%b0%e5%ad%a6%e5%8e%9f%e7%90%86></a></h3><p>在样本空间中，划分超平面可通过以下线性方程来描述：</p><p>$$w^Tx+b = 0$$</p><p>其中，$w = (w_1;w_2;&mldr;;w_d)$ 为<strong>法向量</strong>，决定了超平面的方向；$b$ 为<strong>位移项</strong>，决定了超平面与原点之间的距离。显然，划分超平面 $(w, b)$ 可被法向量 $w$ 和位移 $b$ 确定。</p><p>样本空间中任意点 $x$ 到超平面 $(w,b)$ 的距离可写为</p><p>$$r=\frac{|w^Tx+b|}{\|w\|}$$</p><p>假设超平面 $(w,b)$ 能将训练样本正确分类，即对于 $(x_i, y_i) \in D$，若 $y_i = +1$，则有 $w^Tx_i+b>0$；若 $y_i = -1$，则有 $w^Tx+b &lt; 0$。令</p><p>$$
\begin{equation}
\begin{cases}
w^Tx_i+b \geq +1, & y_i=+1 \\ w^Tx_i+b \leq -1, & y_i=-1
\end{cases}
\end{equation}
$$</p><p>总存在缩放变换使得上式成立。可以理解为构造了一个安全间距。</p><p>显然，<strong>支持向量</strong>使得上式中的等号成立。它们到超平面的距离之和即<strong>间隔</strong>，为</p><p>$$\gamma = \frac{2}{\| w \|}$$</p><p>我们的目标是找到具有最大间隔的划分超平面，即找到满足 $y_i(w^Tx_i+b) \geq 1, i = 1, 2, &mldr;, m$ 的参数 $w$ 和 $b$，使得 $\gamma$ 最大，即等价于最小化 $\frac{1}{2}\| w \|^2$。</p><h3 id=核函数>核函数
<a class=header-anchor href=#%e6%a0%b8%e5%87%bd%e6%95%b0></a></h3><h4 id=思想>思想
<a class=header-anchor href=#%e6%80%9d%e6%83%b3></a></h4><p>可以用带有**核函数（kernal）**的支持向量机构造复杂的非线性分类器。我们需要定义一个类似于度量相似性的函数，以高斯核函数为例：</p><p>$$
\begin{equation}
\begin{split}
f_1 &= similarity(x, l^{(1)}) \\
&= exp \Big( -\frac{\| x-l^{(1)} \|^2}{2\sigma^2} \Big) \\
&= exp \Big( -\frac{\sum^n_{j=1}(x_j - l^{(1)}_j)^2}{2\sigma^2} \Big)
\end{split}
\end{equation}
$$</p><p>其中，$\sigma^2$ 是高斯核函数的参数。当 $x$ 约等于 $l^{(1)}$ 时，$f_1$ 约等于 1；而当 $x$ 和 $l^{(1)}$ 较远时，$f_1$ 约等于 0。</p><p>通过相似度函数，$l_1$，$l_2$，&mldr; ，$l_j$ 每一个标记会定义一个新特征 $f_1$，$f_2$，&mldr;，$f_j$。假设有 $\theta_0 + \theta_1 f_1 + \theta_2 f_2 + &mldr; + \theta_j f_j \geq 0$，我们由此将复杂的非线性决策边界转换为线性。</p><h4 id=选取标记点>选取标记点
<a class=header-anchor href=#%e9%80%89%e5%8f%96%e6%a0%87%e8%ae%b0%e7%82%b9></a></h4><p>每个标记点都和每个样本点位置重合，即 $l^{(m)} = x^{(m)}$。则有</p><p>$$f_m^{(i)} = sim(x^{(i)}, l^{(m)})$$</p><p>$$f^{(i)} = [f_0^{(i)}, f_1^{(i)}, &mldr; ,f_m^{(i)}]^T$$</p><p>$f^{(i)}$ 就是特征向量。</p><p>因此，包含核函数的 SVM：</p><p>给定 $x$，计算出特征 $f \in \mathbb{R}^{m+1}$，当 $\theta^Tf \geq 0$ 时，预测 $y = 1$。则目标为</p><p>$$min_{\theta}C\sum^m_{i=1}\Big[ y^{(i)}cost_1(\theta^Tf^{(i)}) + (1-y^{(i)})cost_0(\theta^Tf^{(i)}) \Big] + \frac{1}{2}\sum^m_{j=1}\theta^2_j$$</p><p>一个数学细节是 $\sum_j \theta^2_j$ 可以被写为 $\theta^T \theta$（忽略 $\theta_0$）。而在实现时，通常用 $\theta^T M\theta$，$M$ 是一个依赖于所使用的核函数的矩阵，这是一种略有区别的距离度量方法。这使得 SVM 的实现更有效率。</p><p>核函数的思想可以用于例如 Logistic 回归的其他分类器，但是运行十分缓慢，不能使用高级优化技巧。</p><h4 id=参数选择>参数选择
<a class=header-anchor href=#%e5%8f%82%e6%95%b0%e9%80%89%e6%8b%a9></a></h4><p>对于参数 $C$，其作用近似于 $\frac{1}{\lambda}$。</p><ul><li>较大的 $C$：意味着不使用正则化，低偏差高方差</li><li>较小的 $C$：偏向于欠拟合，高偏差低方差</li></ul><p>$\sigma^2$：</p><ul><li>偏大时：特征 $f_i$ 更为平滑，高偏差低方差</li><li>偏小时：特征 $f_i$ 更不平滑，低偏差高方差</li></ul><h3 id=使用-svm>使用 SVM
<a class=header-anchor href=#%e4%bd%bf%e7%94%a8-svm></a></h3><p>使用 SVM 时，需要确定参数 $C$ 和核函数。</p><p>可以不使用核函数，也称使用线性核函数，则 $\theta^Tx \geq 0$。当特征数目较多而样本数较少时，这样可以避免去在一个非常高维的空间中拟合过于复杂的非线性函数，减缓过拟合。</p><p>当特征数目较少而训练样本较多时，想用核函数拟合相当复杂的非线性决策边界，高斯核函数是个不错的选择。如果决定使用高斯核函数，还需要选择 $\sigma^2$。</p><p>如果有大小很不一样的特征变量，在使用高斯核函数前，要注意将特征变量大小按比例归一化。</p><p>最常用的两个核函数是线性核函数和高斯核函数。不是所有提出来的相似性函数都是有效的，需要满足默塞尔定理（Mercer&rsquo;s Theorem），使得用于求解 $\theta$ 的数值优化技巧能够使用。</p><p>其他可以使用的核函数有：</p><ul><li>多项式函数（Polynomial kernel）：$k(x, l) = (X^Tl + m)^n$。通常效果较差，常用于 $x$ 和 $l$ 都是严格的非负数。</li><li>字符串核函数（String kernel）：用于输入数据是字符串时。</li><li>卡方核函数（chi-square kernel）</li><li>直方相交核函数（histogram kernel）</li></ul><p>如何选择使用 Logistic 回归还是 SVM？</p><ul><li>当特征数量远大于训练样本数，一般使用 Logistic 回归，或使用线性核函数的 SVM。</li><li>当特征数量较少，训练样本数中等时，一般使用带高斯核函数的 SVM。</li><li>当特征数量较少，训练样本数非常多（一般大于十万），一般先增加更多特征变量，然后使用 Logistic 回归或者使用线性核函数的 SVM。</li></ul><p>而设计较好的神经网络可能能获得较好的效果，但训练速度较慢。并且 SVM 的优化是一种凸优化问题，不会有困扰神经网络的局部最优问题。</p><h2 id=无监督学习-1>无监督学习
<a class=header-anchor href=#%e6%97%a0%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0-1></a></h2><p>在监督学习中，我们有一系列标签，然后用假设函数去拟合它。作为对比，在无监督学习中，数据不带有标签。我们通过这些数据找到一些隐含在数据中的结构。聚类算法是一种典型的无监督学习算法。</p><h3 id=k-means-算法>K-Means 算法
<a class=header-anchor href=#k-means-%e7%ae%97%e6%b3%95></a></h3><p>K-means 是一种经典的聚类算法，其需要的输入包括聚类数目 $K$ 和训练集 $\big\{x^{(1)}, x^{(2)}, &mldr;, x^{(m)}\big\}$。</p><p>步骤：</p><ol><li>随机初始化 $K$ 个聚类中心 $\mu_1, \mu_2, &mldr;, \mu_K \in \mathbb{R}^{n}$；</li><li>将训练集中的每个数据点分配给离它最近的聚类中心，最终形成 $K$ 个聚类；</li><li>对于每个聚类，将聚类中所有数据点的平均点作为新的聚类中心；</li><li>重复第 2 步和第 3 步直到收敛。</li></ol><p>可能在过程中出现没有任何数据点的聚类中心，这种情况下一般直接移除，或者如果确实需要 $K$ 个聚类，则可以重新随机初始化一个聚类中心。</p><h4 id=优化目标>优化目标
<a class=header-anchor href=#%e4%bc%98%e5%8c%96%e7%9b%ae%e6%a0%87></a></h4><p>规定下列符号：</p><ul><li>$c^{(i)}$：$x^{(i)}$ 被分配给的聚类的序号；</li><li>$\mu_k$：第 $k$ 个聚类中心；</li><li>$\mu_{c^{(i)}}$：$x^{(i)}$ 被分配给的聚类的中心。</li></ul><p>则有代价函数作为优化目标：</p><p>$$J(c^{(1)}, &mldr; , c^{(m)}, \mu_1, &mldr; , \mu_K) = \frac{1}{m}\sum^m_{i=1} \| x^{(i)} - \mu_{c^{(i)}} \|^2$$</p><p>因此，K-means 算法就是要找到能使 $J(c^{(1)}, &mldr; , c^{(m)}, \mu_1, &mldr; , \mu_K)$ 最小的参数 $c^{(i)}$ 和 $\mu_K$。</p><h4 id=随机初始化>随机初始化
<a class=header-anchor href=#%e9%9a%8f%e6%9c%ba%e5%88%9d%e5%a7%8b%e5%8c%96></a></h4><p>一个更好的策略是不再从空间中随机选择点，而是从训练数据中随机选择以初始化聚类中心。</p><p>K-means 算法最终可能因为选取了不同的初始化点而收敛得到不同的结果。因此，可能落到局部最优。我们可以尝试多次随机初始化，并选择能够使代价函数最小的聚类结果。</p><h4 id=选择聚类数量>选择聚类数量
<a class=header-anchor href=#%e9%80%89%e6%8b%a9%e8%81%9a%e7%b1%bb%e6%95%b0%e9%87%8f></a></h4><p>通过改变 $K$ 值来得到一张代价函数下降的曲线图，并选择曲线的拐点的横坐标作为合适的 $K$ 值被称为“肘部法则（Elbow method）”，但不是所有时候得到的曲线都有明显的拐点。</p><h2 id=降维>降维
<a class=header-anchor href=#%e9%99%8d%e7%bb%b4></a></h2><p>**降维（dimensionality reduction）**是指通过某种数学变换将原始高维属性空间转变为一个低维“子空间”，在这个子空间中样本密度大幅提高，以避免在高维情形下出现的数据样本稀疏、距离计算困难等问题。降维也是无监督学习问题的一种。</p><p>降维的两种应用：</p><ul><li>数据压缩：对数据进行压缩，占用较少存储空间，并加速学习算法</li><li>数据可视化</li></ul><h3 id=主成分分析pca>主成分分析（PCA）
<a class=header-anchor href=#%e4%b8%bb%e6%88%90%e5%88%86%e5%88%86%e6%9e%90pca></a></h3><p>**主成分分析（Pricipal Component Analysis，PCA）**是最常用的一种降维算法。</p><p>PCA 会试图寻找一个低维的超平面，对数据进行投影，并最小化投影误差（数据点到该平面的距离）。在应用 PCA 之前，常规做法是先进行均值归一化和特征规范化。</p><p>输入样本集和降维后低维空间的维数 k 值，PCA 算法的过程为：</p><ol><li>对所有样本进行中心化：$x_i \leftarrow x_i - \frac{1}{m}\sum^m_{i=1}x_i$ ；</li><li>计算样本的协方差矩阵 $XX^T$；</li><li>对协方差矩阵 $XX^T$ 做特征值分解；</li><li>取最大的 $k$ 个特征值所对应的特征向量 $w_1, w_2, &mldr;, w_k$。</li></ol><p>则超平面 $W = (w_1, w_2, &mldr;, w_k)$。</p><h4 id=选取-k-值>选取 k 值
<a class=header-anchor href=#%e9%80%89%e5%8f%96-k-%e5%80%bc></a></h4><p>平均投影误差平方：</p><p>$$\frac{1}{m}\sum^m_{i=1} \| x^{(i)} - x^{(i)}_{approx} \|^2$$</p><p>数据的总方差：</p><p>$$\frac{1}{m}\sum^m_{i=1} \| x^{(i)} \|^2$$</p><p>因此选取较小的 k 值，使得：</p><p>$$\frac{\frac{1}{m}\sum^m_{i=1} \| x^{(i)} - x^{(i)}_{approx} \|^2}{\frac{1}{m}\sum^m_{i=1} \| x^{(i)} \|^2} \leq 0.01$$</p><p>这样，99% 的方差会被保留。</p><p>实现时，通常通过对协方差矩阵进行奇异值分解，将求得的特征值排序：$\lambda_1、\lambda_1、&mldr;、\lambda_m$，然后取 k 值使：</p><p>$$\frac{\sum^k_{i=1}S_{ii}}{\sum^m_{i=1}S_{ii}} \geq 0.99$$</p><h3 id=压缩重现>压缩重现
<a class=header-anchor href=#%e5%8e%8b%e7%bc%a9%e9%87%8d%e7%8e%b0></a></h3><p>$$z = U^T_{reduce}x$$</p><p>因此我们可以从低维数据重现一个近似的高维数据：</p><p>$$x \approx x_{approx} = U_{reduce}z$$</p><p>我们称这一过程为原始数据的<strong>重构（reconstruction）</strong>。</p><h3 id=应用-pca-的建议>应用 PCA 的建议
<a class=header-anchor href=#%e5%ba%94%e7%94%a8-pca-%e7%9a%84%e5%bb%ba%e8%ae%ae></a></h3><p>用 PCA 可以降低数据的维度，从而使得算法运行更加高效。</p><ol><li>从训练集中只抽取输入的向量，不要标签；</li><li>使用 PCA 得到原始数据的低维表示；</li><li>现在我们得到一个新的训练集。</li></ol><p>注意：从 $x^{(i)}$ 到 $z^{(i)}$ 的映射需要通过**仅在训练集上运行 PCA **得到，不过这个映射关系同样可用于验证集和测试集。</p><p>一种错误的认知是可以使用 PCA 来防止过拟合。有些人会认为因为 PCA 能够降低数据维度，因此更少的特征数意味着更小的可能性导致过拟合。实际上，PCA 会舍弃一些有价值的信息，使用 PCA 来防止过拟合不是一种好的应用。使用正则化是更好的防止过拟合的方法。</p><p>在设计机器学习系统时，你可以先在原始数据上尝试运行算法，之后根据效果来决定是否使用 PCA 来加速学习或者压缩数据以减少所需的存储空间。</p><h2 id=异常检测>异常检测
<a class=header-anchor href=#%e5%bc%82%e5%b8%b8%e6%a3%80%e6%b5%8b></a></h2><p>**异常检测（Anomaly Detection）**是机器学习算法的一个常见应用。这是一个无监督学习问题，会通过新的数据特征落在已有数据的特征分布的概率来判断新的数据是否隐含了某种异常。</p><h3 id=高斯分布>高斯分布
<a class=header-anchor href=#%e9%ab%98%e6%96%af%e5%88%86%e5%b8%83></a></h3><p>高斯分布（Gaussian ditribution），即正态分布（Normal ditribution）。</p><p>假设 $x \in \mathbb{R}$，如果 $x$ 的概率分布服从高斯分布，其中均值为 $\mu$，方差为 $\sigma^2$，则写作</p><p>$$x \sim \mathcal{N}(\mu, \sigma^2)$$</p><p>概率分布的公式为：</p><p>$$p(x;\mu, \sigma^2) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})$$</p><p>参数估计（parameter estimation）问题是，给定数据集确定满足高斯分布，需要估计出 $\mu$ 和 $\sigma$。计算公式为：</p><p>$$\mu = \frac{1}{m}\sum^m_{i=1}x^{(i)}$$</p><p>$$\sigma^2 = \frac{1}{m}\sum^m_{i=1}(x^{(i)} - \mu)^2$$</p><p>其中，$\frac{1}{m}$ 有时也写作 $\frac{1}{m-1}$，在实践中区别不大。</p><h3 id=算法>算法
<a class=header-anchor href=#%e7%ae%97%e6%b3%95></a></h3><p>假设有一个训练集 $\big\{ x^{(1)},&mldr;,x^{(m)} \big\}$，每个样本 $x \in \mathbb{R}^n$。</p><p>在特征相互独立的假设下，有</p><p>$$p(x) = \prod^n_{j=1}p(x_j;\mu_j,\sigma_j^2)$$</p><p>如果 $p(x) &lt; \varepsilon$，认为异常。$\varepsilon$ 需要选定。</p><p>分布项 $p(x)$ 的估计问题有时称为**密度估计（Density estimation）**问题。</p><h3 id=开发和评估异常检测系统>开发和评估异常检测系统
<a class=header-anchor href=#%e5%bc%80%e5%8f%91%e5%92%8c%e8%af%84%e4%bc%b0%e5%bc%82%e5%b8%b8%e6%a3%80%e6%b5%8b%e7%b3%bb%e7%bb%9f></a></h3><p>如果有一些带标签的数据，来指明哪些是正常数据，哪些是异常数据，则这些数据可以用于评估算法。</p><p>我们这样划分训练集、验证集和测试集：</p><ul><li>训练集：正常无标签数据</li><li>验证集和测试集：包含少量已知是异常的数据</li></ul><p>系统开发步骤：</p><ol><li>使用训练集拟合模型 $p(x)$；</li><li>对于验证集和测试集用模型进行预测；</li><li>因为数据类别比例相差较大，可以用 F1-Score 等来评估算法。</li></ol><p>选择 $\varepsilon$ 的一种方法是使用多个值，最后选择能够最大化 F1-Score 的 $\varepsilon$ 值。</p><h3 id=异常检测-vs-监督学习>异常检测 VS 监督学习
<a class=header-anchor href=#%e5%bc%82%e5%b8%b8%e6%a3%80%e6%b5%8b-vs-%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0></a></h3><p>异常检测与监督学习的区别：</p><ol><li>异常检测中样本比例不平衡，而监督学习一般有比例相近的正负样本；</li><li>异常检测不追求从样本中分析出异常的具体种类，因为实际中可能出现与以前完全不同的全新异常。而监督学习会划分具体种类。</li></ol><h3 id=选择使用的特征>选择使用的特征
<a class=header-anchor href=#%e9%80%89%e6%8b%a9%e4%bd%bf%e7%94%a8%e7%9a%84%e7%89%b9%e5%be%81></a></h3><ol><li>通过画直方图选择比较符合高斯分布的特征。不太符合的可以通过一些转换（例如取对数）使其符合高斯分布。</li><li>选择异常样本中值相对于正常样本概率较低的特征。这样，可以使异常样本得到相对较低的 $p(x)$。反映到实际中选择，可以是异常样本中某特征值异常地大或异常地校。</li></ol><h3 id=多元高斯分布>多元高斯分布
<a class=header-anchor href=#%e5%a4%9a%e5%85%83%e9%ab%98%e6%96%af%e5%88%86%e5%b8%83></a></h3><p>**多元高斯分布（Multivariate Gaussian distribution）**有两个参数 $\mu$ 和 $\Sigma$，$\mu \in \mathbb{R}$，$\Sigma \in \mathbb{R}^2$。</p><p>$$p(x;\mu, \Sigma) = \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}exp \Big( -\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)\Big)$$</p><p>参数估计：</p><p>$$\mu = \frac{1}{m}\sum^m_{i=1}x^{(i)}$$</p><p>$$\Sigma = \frac{1}{m}\sum^m_{i=1}(x^{(i)}-\mu)(x^{(i)}-\mu)^T$$</p><p>使用原先的高斯分布的模型需要创建新的特征来捕捉异常的特征组合值。而多元高斯分布能够自动捕捉不同特征之间的关系；但原始模型的计算成本较低，可以使用数量巨大的特征。而多元高斯分布需要计算协方差矩阵 $Sigma$ 的逆矩阵，因此计算成本较高的同时，需要保证样本数量大于特征数量，来保证 $Sigma$ 可逆。</p><h2 id=推荐系统>推荐系统
<a class=header-anchor href=#%e6%8e%a8%e8%8d%90%e7%b3%bb%e7%bb%9f></a></h2><h3 id=基于内容的推荐算法>基于内容的推荐算法
<a class=header-anchor href=#%e5%9f%ba%e4%ba%8e%e5%86%85%e5%ae%b9%e7%9a%84%e6%8e%a8%e8%8d%90%e7%ae%97%e6%b3%95></a></h3><p>一种做法是，我们将每部电影中爱情成分、动作成分等评值作为特征，而某用户对不同电影的打分作为标签。这样，为了做出预测，可以看作一个线性回归问题，对于用户 $j$，预测其给电影 $i$ 评分为 $(\theta^{(j)})^Tx^{(i)}$。</p><p>更正式的问题描述：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://raw.githubusercontent.com/bighuang624/pic-repo/master/problem-formulation-of-content-based-recommendation-algorithm1.png alt></p><p>当学习 $\theta^{(1)}, \theta^{(2)}, \dots, \theta^{(n_u)}$ 时，优化目标为：</p><p>$$
\min_{\theta^{(1)},\dots,\theta^{(n_u)}} \frac{1}{2} \sum_{j=1}^{n_u} \sum_{i: r(i,j) = 1} \left( (\theta^{(j)})^T x^{(i)} - y^{(i,j)} \right)^2 + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^{n} (\theta^{(j)}_k)^2
$$</p><p>通过梯度下降来更新参数：</p><p>当 $k=0$ 时：
$$
\theta^{(j)}_k := \theta^{(j)}<em>k - \alpha \sum</em>{i: r(i,j) = 1} \left( (\theta^{(j)})^T x^{(i)} - y^{(i,j)} \right) x^{(i)}_k
$$</p><p>当 $k \neq 0$ 时：
$$
\theta^{(j)}_k := \theta^{(j)}<em>k - \alpha \left( \sum</em>{i: r(i,j) = 1} \left( (\theta^{(j)})^T x^{(i)} - y^{(i,j)} \right) x^{(i)}_k + \lambda \theta^{(j)}_k \right)
$$
两者的区别是因为一般不将偏置项 $b$ 计算在正则化项中。</p><h3 id=协同过滤基本思想>协同过滤基本思想
<a class=header-anchor href=#%e5%8d%8f%e5%90%8c%e8%bf%87%e6%bb%a4%e5%9f%ba%e6%9c%ac%e6%80%9d%e6%83%b3></a></h3><p>实际上，获取电影的众多特征需要花费很高的人力。我们可以反向来思考这个问题，假设我们拥有了用户的偏好向量 $\theta^{(j)}$，为了学习 $x^{(i)}$，有：</p><p>$$min_{x^{(i)}}\frac{1}{2}\sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2}\sum^n_{k=1}(x_k^{(i)})^2$$</p><p>学习 $x^{(1)}, x^{(2)}, &mldr;, x^{(n_u)}$ 时，则优化目标为：</p><p>$$min_{x^{(1)}, x^{(2)}, &mldr;, x^{(n_m)}}\frac{1}{2}\sum^{n_m}_{i=1}\sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2}\sum^{n_m}_{i=1}\sum^n_{k=1}(x_k^{(i)})^2$$</p><p>这被称为<strong>协同过滤（Collaborative filtering）</strong>。</p><h3 id=协同过滤算法>协同过滤算法
<a class=header-anchor href=#%e5%8d%8f%e5%90%8c%e8%bf%87%e6%bb%a4%e7%ae%97%e6%b3%95></a></h3><p>为了更高效地解得 $\theta$ 和 $x$，我们将两个代价函数合为一个：</p><p>$$J(x^{(1)}, x^{(2)}, &mldr;, x^{(n_u)}, \theta^{(1)}, \theta^{(2)}, &mldr; ,\theta^{(n_u)}) = \frac{1}{2}\sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2}\sum^{n_m}_{i=1}\sum^n_{k=1}(x_k^{(i)})^2+ \frac{\lambda}{2}\sum^{n_m}_{i=1}\sum^n_{k=1}(x_k^{(i)})^2$$</p><p>$$min_{x^{(1)}, x^{(2)}, &mldr;, x^{(n_u)}, \theta^{(1)}, \theta^{(2)}, &mldr; ,\theta^{(n_u)}}J(x^{(1)}, x^{(2)}, &mldr;, x^{(n_u)}, \theta^{(1)}, \theta^{(2)}, &mldr; ,\theta^{(n_u)})$$</p><p>总结一下协同过滤算法：</p><ol><li>将 $x^{(1)}, x^{(2)}, &mldr;, x^{(n_u)}, \theta^{(1)}, \theta^{(2)$ 初始化为较小的任意值；</li><li>用梯度下降来最小化 $J(x^{(1)}, x^{(2)}, &mldr;, x^{(n_u)}, \theta^{(1)}, \theta^{(2)}, &mldr; ,\theta^{(n_u)})$；</li><li>用某个用户的参数 $\theta$ 和某部电影的特征 $x$ 来预测他会给这部他没看过的电影打分为 $\theta^Tx$。</li></ol><h3 id=矢量化低秩矩阵分解>矢量化：低秩矩阵分解
<a class=header-anchor href=#%e7%9f%a2%e9%87%8f%e5%8c%96%e4%bd%8e%e7%a7%a9%e7%9f%a9%e9%98%b5%e5%88%86%e8%a7%a3></a></h3><p><img src=/imgs/img-lazy-loading.gif data-src=https://raw.githubusercontent.com/bighuang624/pic-repo/master/collaborative-filtering.png alt></p><p>我们还可以通过特征来找到相关的电影，尽管这些特性可能不具有可解释性。</p><h3 id=实施细节均值规范化>实施细节：均值规范化
<a class=header-anchor href=#%e5%ae%9e%e6%96%bd%e7%bb%86%e8%8a%82%e5%9d%87%e5%80%bc%e8%a7%84%e8%8c%83%e5%8c%96></a></h3><p>对于一个新用户或者没有给任何电影评过分的用户，显然上述方法只会预测这名用户会给所有电影打 0 分。因此，我们要对每部电影的评分做一个均一化：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://raw.githubusercontent.com/bighuang624/pic-repo/master/mean-normalization-of-collaborative-filtering.png alt></p><h2 id=大规模机器学习>大规模机器学习
<a class=header-anchor href=#%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0></a></h2><h3 id=随机梯度下降>随机梯度下降
<a class=header-anchor href=#%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d></a></h3><p>当我们每次使用全部的数据来进行梯度下降时，有公式：</p><p>$$\theta_j :=\theta_j - \alpha \frac{1}{m}\sum^m_{i=1}(h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$$</p><p>每次求和的计算量太大，迭代速度慢，计算机的内存也可能不能支持所有数据。这种方法被称为<strong>批量梯度下降（Batch gradient descent）</strong>。</p><p>与此相对，有<strong>随机梯度下降（Stochastic gradient descent）</strong>，在随机打乱所有数据后，每次只用一个训练数据来进行梯度下降以更新参数。这样，收敛速度更快。尽管这样做可能很难完全收敛，但可以非常接近全局最小值。</p><h3 id=mini-batch-梯度下降>Mini-batch 梯度下降
<a class=header-anchor href=#mini-batch-%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d></a></h3><p><strong>Mini-batch 梯度下降</strong>每次迭代会使用 b 个样本。b 是一个称为 mini-batch 大小的参数，通常选取范围为 2 - 100。</p><p>使用 Mini-batch 梯度下降有时会比随机梯度下降有更快的收敛速度，这是因为在向量化后，可以合理使用并行运算。</p><p>每一个 Mini-batch 再计算一次 $J_{train}(\theta)$ 并和之前进行比对，可以确认梯度下降在正确执行，同时可以考虑将学习率调小使收敛更充分。</p><h3 id=在线学习>在线学习
<a class=header-anchor href=#%e5%9c%a8%e7%ba%bf%e5%ad%a6%e4%b9%a0></a></h3><p>**在线学习机制（Online learning setting）**可以支持有大量用户流的网站来快速调整模型，适应用户的喜好变化。每次将新鲜的用户数据用于梯度下降以调整参数，这些数据不用保存，用完即可丢弃。</p><h3 id=mapreduce>MapReduce
<a class=header-anchor href=#mapreduce></a></h3><p>MapReduce 指均分训练集，将每个子集分发给一台主机并行计算，最后结果汇总到一台机器，以加快计算速度。只要学习算法可以表示成一系列的求和形式，或者表示成在训练集上对函数的求和形式，就可以使用 MapReduce 技巧。把主机换成多核 CPU 的每个核同理。</p></div><footer class=post-footer><div class=post-tags><a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></div><div class=post-share-tools><div class=post-share-loading><i class="fa-solid fa-ellipsis fa-spin"></i></div><div class="a2a_kit a2a_kit_size_32 a2a_default_style"><a class=a2a_dd href=https://www.addtoany.com/share></a><a class=a2a_button_wechat></a><a class=a2a_button_qzone></a><a class=a2a_button_sina_weibo></a><a class=a2a_button_douban></a><a class=a2a_button_facebook></a><a class=a2a_button_x></a><a class=a2a_button_email></a><a class=a2a_button_printfriendly></a></div></div><hr><div class=post-copyright><img src=/imgs/cc/cc.svg width=75 height=75 align=right alt=共享知识><ul><li class=post-copyright-title><strong>文章标题：</strong>
机器学习(1)</li><li class=post-copyright-author><strong>本文作者： </strong>Cita</li><li class=post-copyright-link><strong>本文链接：</strong>
<a id=post-cr-link href=/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/ title=机器学习(1)>/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/</a></li><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <i class="fab fa-fw fa-creative-commons"></i><a target=_blank href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/post/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E5%8C%96%E4%B8%8E%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/ rel=next title=傅里叶变换与微分方程><i class="fa fa-chevron-left"></i> 傅里叶变换与微分方程</a></div><div class="post-nav-prev post-nav-item"><a href=/post/%E6%B5%85%E6%9E%90pid/ rel=prev title=浅析PID>浅析PID
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div id=comments class=post-comments><div class=comment-head><div class=comment-headline><i class="fas fa-comments fa-fw"></i>
<span>评论交流</span></div></div><div class=comment-wrap><div><div class=comment-loading><i class="fa fa-sync fa-spin"></i></div><div class=waline3-container></div></div></div></div></div></main><footer class=footer><div class=footer-inner><div id=i18n-translate class=i18n-translate><i class="fa fa-language"></i><div id=lang-select class=lang-select><div id=lang-selected class=selected-option><span class="flag-icon flag-icon-zh-cn"></span>
<span class=selected-language>简体中文</span>
<i class="fa fa-chevron-down"></i></div><div id=lang-options class=lang-options><div class=lang-option lang-code=zh-cn lang-name=简体中文 lang-url=/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/><span class="flag-icon flag-icon-zh-cn"></span>
<span class=lang-name>简体中文</span></div></div></div></div><div class=copyright>&copy;
<span itemprop=copyrightYear>2024 - 2025
</span><span class=with-love><i class="fa fa-heart"></i>
</span><span class=author itemprop=copyrightHolder>Cita</span></div><div class=vendors-list><a target=_blank href=https://github.com title=Github><img src=/imgs/img-lazy-loading.gif data-src=/imgs/vendors/github.svg alt=Github>
</a><span>提供CDN/云资源支持</span></div></div></footer><script class=next-config data-name=page type=application/json>{"comments":true,"expired":true,"expiredTips":{"info":"🕰️ 嗨，这篇文章已是#前的陈年佳酿，信息可能需要更新，阅读前请检查最佳赏味期限。","warn":"🚀 注意啦！这篇文章来自#以前，可能已跟不上时代的步伐。阅读时请自备时光机，小心穿越哦！"},"isHome":false,"isPage":true,"math":{"js":{"file":"es5/tex-mml-svg.js","name":"mathjax","version":"3.2.2"},"render":"mathjax"},"path":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01","permalink":"/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/","title":"机器学习(1)","toc":true,"waline3":{"commentcnt":{"alias":"@waline/client","alias_name":"waline","file":"dist/comment.js","name":"comment","version":"3.3.0"},"pagecnt":{"alias":"@waline/client","alias_name":"waline","file":"dist/pageview.js","name":"pageview","version":"3.3.0"}}}</script><script type=text/javascript src=/js/3rd/animejs/3.2.2/anime.min.js crossorigin=anonymous defer></script><script type=text/javascript src=/js/3rd/viewerjs/1.11.6/viewer.min.js crossorigin=anonymous defer></script><script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":true,"save":"manual"},"copybtn":true,"darkmode":false,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"找到 ${hits} 个搜索结果","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"isMultiLang":true,"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"limit":1e3,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"postmeta":{"comments":{"enable":true,"plugin":"waline3"},"views":{"enable":true,"plugin":"waline3"}},"root":"/","scheme":"Gemini","share":{"addtoany":{"js":"https://static.addtoany.com/menu/page.js","locale":"zh-CN","num":8},"enable":true},"sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"vendor":{"plugins":"local","router":{"name":"local","type":"modern","url":"/js/3rd"}},"version":"4.7.2","waline3":{"cfg":{"emoji":true,"imguploader":true,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o 可用快捷键选取表情符号：😀😄😁🥳👻👽👀🚄 (Window系统：Win+.，Mac系统：Control+Command+Space)","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"search":true,"serverurl":"https://walinejs.comment.lithub.cc","sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"@waline/client","file":"dist/waline.min.css","name":"waline","version":"3.3.0"},"js":{"alias":"@waline/client","file":"dist/waline.js","name":"waline","version":"3.3.0"}}}</script><script type=text/javascript src="/js/main.min.js?=1751594388" defer></script><script type=text/javascript src="/js/math.min.js?=1751594388" defer></script></body></html>